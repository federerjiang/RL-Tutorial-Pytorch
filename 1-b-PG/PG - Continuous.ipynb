{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gym\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# # Imports specifically so we can render outputs in Jupyter.\n",
    "# from JSAnimation.IPython_display import display_animation\n",
    "# from matplotlib import animation\n",
    "# from IPython.display import display\n",
    "\n",
    "\n",
    "# def display_frames_as_gif(frames):\n",
    "#     \"\"\"\n",
    "#     Displays a list of frames as a gif, with controls\n",
    "#     \"\"\"\n",
    "#     #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "#     patch = plt.imshow(frames[0])\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     def animate(i):\n",
    "#         patch.set_data(frames[i])\n",
    "\n",
    "#     anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "#     display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('MountainCarContinuous-v0')\n",
    "\n",
    "# # Run a demo of the environment\n",
    "# observation = env.reset()\n",
    "# cum_reward = 0\n",
    "# frames = []\n",
    "# for t in range(5000):\n",
    "#     # Render into buffer. \n",
    "#     frames.append(env.render(mode = 'rgb_array'))\n",
    "#     action = env.action_space.sample()\n",
    "#     observation, reward, done, info = env.step(action)\n",
    "#     if done:\n",
    "#         break\n",
    "# env.render(close=True)\n",
    "# display_frames_as_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Environment Intro\n",
    "Ref: https://gym.openai.com/envs/MountainCarContinuous-v0/\n",
    "- Description: A car is on a one-dimensional track, positioned between two \"mountains\". The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum. Here, the reward is greater if you spend less energy to reach the goal\n",
    "\n",
    "- Observation: **position** and **velocity**\n",
    "- Action: **force [-1.0..1.0]**\n",
    "- Reward:\n",
    "---\n",
    "    done = bool(position >= self.goal_position)\n",
    "    reward = 0\n",
    "    if done:\n",
    "        reward = 100.0\n",
    "    reward-= math.pow(action[0],2)*0.1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-05-31 09:36:36,490] Making new env: MountainCarContinuous-v0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.43852191  0.        ]\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(1,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09762701] [-0.43900664 -0.00048473] -0.0009531032662651797 False {}\n",
      "[0.43037873] [-0.43947346 -0.00046682] -0.018522585359905353 False {}\n",
      "[0.20552675] [-0.44025625 -0.00078279] -0.004224124584656844 False {}\n",
      "[0.08976637] [-0.44152296 -0.00126672] -0.0008058000463731728 False {}\n",
      "[-0.1526904] [-0.44362808 -0.00210512] -0.002331435865593162 False {}\n",
      "[0.29178823] [-0.44588956 -0.00226148] -0.008514036891002495 False {}\n",
      "[-0.12482558] [-0.44891584 -0.00302627] -0.0015581424791871108 False {}\n",
      "[0.783546] [-0.45132224 -0.0024064 ] -0.0613944336567182 False {}\n",
      "[0.92732552] [-0.45287549 -0.00155325] -0.08599326219017395 False {}\n",
      "[-0.23311696] [-0.45530486 -0.00242938] -0.005434351813456613 False {}\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    if done:\n",
    "        env.reset()\n",
    "    print(action, observation, reward, done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Gradient - REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Policy Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(2,8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        self.fc3_ = nn.Linear(4, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.fc3(x)\n",
    "        sigma_ = self.fc3_(x)\n",
    "        return mu, sigma_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, reward -38.7\n",
      "Episode: 1, reward -73.2\n",
      "Episode: 2, reward -92.2\n",
      "Episode: 3, reward -95.1\n",
      "Episode: 4, reward -97.0\n",
      "Episode: 5, reward 24.5\n",
      "Episode: 6, reward -96.8\n",
      "Episode: 7, reward 27.3\n",
      "Episode: 8, reward -97.3\n",
      "Episode: 9, reward -98.5\n",
      "Episode: 10, reward -97.5\n",
      "Episode: 11, reward 60.9\n",
      "Episode: 12, reward 13.1\n",
      "Episode: 13, reward 13.6\n",
      "Episode: 14, reward -98.1\n",
      "Episode: 15, reward -98.1\n",
      "Episode: 16, reward -98.1\n",
      "Episode: 17, reward -97.8\n",
      "Episode: 18, reward 34.2\n",
      "Episode: 19, reward -98.7\n",
      "Episode: 20, reward -99.0\n",
      "Episode: 21, reward -98.9\n",
      "Episode: 22, reward -99.1\n",
      "Episode: 23, reward -99.2\n",
      "Episode: 24, reward 11.1\n",
      "Episode: 25, reward -99.3\n",
      "Episode: 26, reward -99.0\n",
      "Episode: 27, reward -99.2\n",
      "Episode: 28, reward 18.1\n",
      "Episode: 29, reward -99.4\n",
      "Episode: 30, reward -99.4\n",
      "Episode: 31, reward -99.5\n",
      "Episode: 32, reward 46.2\n",
      "Episode: 33, reward 26.4\n",
      "Episode: 34, reward 30.3\n",
      "Episode: 35, reward -99.3\n",
      "Episode: 36, reward -99.3\n",
      "Episode: 37, reward -99.4\n",
      "Episode: 38, reward -99.5\n",
      "Episode: 39, reward 36.0\n",
      "Episode: 40, reward -99.6\n",
      "Episode: 41, reward -99.5\n",
      "Episode: 42, reward -99.6\n",
      "Episode: 43, reward -99.6\n",
      "Episode: 44, reward -99.4\n",
      "Episode: 45, reward 21.4\n",
      "Episode: 46, reward 10.7\n",
      "Episode: 47, reward -99.6\n",
      "Episode: 48, reward 35.7\n",
      "Episode: 49, reward -99.7\n",
      "Episode: 50, reward -99.6\n",
      "Episode: 51, reward 22.9\n",
      "Episode: 52, reward 34.3\n",
      "Episode: 53, reward 7.4\n",
      "Episode: 54, reward 28.3\n",
      "Episode: 55, reward 18.0\n",
      "Episode: 56, reward -99.7\n",
      "Episode: 57, reward -99.5\n",
      "Episode: 58, reward -99.7\n",
      "Episode: 59, reward -99.7\n",
      "Episode: 60, reward -99.6\n",
      "Episode: 61, reward -99.6\n",
      "Episode: 62, reward -99.7\n",
      "Episode: 63, reward -99.6\n",
      "Episode: 64, reward -99.9\n",
      "Episode: 65, reward -99.5\n",
      "Episode: 66, reward -99.7\n",
      "Episode: 67, reward 64.8\n",
      "Episode: 68, reward -99.9\n",
      "Episode: 69, reward -99.7\n",
      "Episode: 70, reward -99.5\n",
      "Episode: 71, reward 32.7\n",
      "Episode: 72, reward -99.8\n",
      "Episode: 73, reward -99.8\n",
      "Episode: 74, reward -99.6\n",
      "Episode: 75, reward -99.9\n",
      "Episode: 76, reward -99.7\n",
      "Episode: 77, reward -99.7\n",
      "Episode: 78, reward 54.5\n",
      "Episode: 79, reward -99.8\n",
      "Episode: 80, reward 12.8\n",
      "Episode: 81, reward 42.1\n",
      "Episode: 82, reward 49.6\n",
      "Episode: 83, reward -99.7\n",
      "Episode: 84, reward 24.5\n",
      "Episode: 85, reward 48.5\n",
      "Episode: 86, reward -99.7\n",
      "Episode: 87, reward 34.3\n",
      "Episode: 88, reward 27.7\n",
      "Episode: 89, reward -99.8\n",
      "Episode: 90, reward 2.5\n",
      "Episode: 91, reward -99.9\n",
      "Episode: 92, reward 27.9\n",
      "Episode: 93, reward 7.8\n",
      "Episode: 94, reward -99.9\n",
      "Episode: 95, reward 25.4\n",
      "Episode: 96, reward -99.7\n",
      "Episode: 97, reward 56.4\n",
      "Episode: 98, reward 15.8\n",
      "Episode: 99, reward -99.8\n",
      "Episode: 100, reward -99.8\n",
      "Episode: 101, reward 13.6\n",
      "Episode: 102, reward 57.2\n",
      "Episode: 103, reward -99.8\n",
      "Episode: 104, reward 5.6\n",
      "Episode: 105, reward -99.6\n",
      "Episode: 106, reward -99.8\n",
      "Episode: 107, reward -99.8\n",
      "Episode: 108, reward 20.0\n",
      "Episode: 109, reward -99.9\n",
      "Episode: 110, reward -99.7\n",
      "Episode: 111, reward -99.8\n",
      "Episode: 112, reward 8.3\n",
      "Episode: 113, reward -99.8\n",
      "Episode: 114, reward 30.6\n",
      "Episode: 115, reward 46.8\n",
      "Episode: 116, reward -99.9\n",
      "Episode: 117, reward 73.5\n",
      "Episode: 118, reward -99.7\n",
      "Episode: 119, reward 25.7\n",
      "Episode: 120, reward -99.8\n",
      "Episode: 121, reward 53.5\n",
      "Episode: 122, reward 30.9\n",
      "Episode: 123, reward -99.8\n",
      "Episode: 124, reward 24.5\n",
      "Episode: 125, reward -99.8\n",
      "Episode: 126, reward -99.9\n",
      "Episode: 127, reward -99.6\n",
      "Episode: 128, reward -99.8\n",
      "Episode: 129, reward -99.8\n",
      "Episode: 130, reward -99.8\n",
      "Episode: 131, reward -99.8\n",
      "Episode: 132, reward -99.8\n",
      "Episode: 133, reward 10.2\n",
      "Episode: 134, reward -99.9\n",
      "Episode: 135, reward -99.9\n",
      "Episode: 136, reward -99.8\n",
      "Episode: 137, reward -99.8\n",
      "Episode: 138, reward 57.3\n",
      "Episode: 139, reward -99.8\n",
      "Episode: 140, reward -99.8\n",
      "Episode: 141, reward 70.0\n",
      "Episode: 142, reward -99.8\n",
      "Episode: 143, reward -99.8\n",
      "Episode: 144, reward 22.9\n",
      "Episode: 145, reward -99.8\n",
      "Episode: 146, reward -99.8\n",
      "Episode: 147, reward -99.8\n",
      "Episode: 148, reward 40.5\n",
      "Episode: 149, reward -99.8\n",
      "Episode: 150, reward 41.7\n",
      "Episode: 151, reward -99.9\n",
      "Episode: 152, reward 56.7\n",
      "Episode: 153, reward -99.8\n",
      "Episode: 154, reward -99.9\n",
      "Episode: 155, reward 80.9\n",
      "Episode: 156, reward 32.7\n",
      "Episode: 157, reward -99.8\n",
      "Episode: 158, reward -99.9\n",
      "Episode: 159, reward -99.9\n",
      "Episode: 160, reward -99.8\n",
      "Episode: 161, reward -99.8\n",
      "Episode: 162, reward 23.2\n",
      "Episode: 163, reward 5.2\n",
      "Episode: 164, reward -99.9\n",
      "Episode: 165, reward 5.7\n",
      "Episode: 166, reward -99.9\n",
      "Episode: 167, reward -99.7\n",
      "Episode: 168, reward 27.0\n",
      "Episode: 169, reward -99.9\n",
      "Episode: 170, reward 48.0\n",
      "Episode: 171, reward 3.7\n",
      "Episode: 172, reward -99.9\n",
      "Episode: 173, reward 45.4\n",
      "Episode: 174, reward -99.8\n",
      "Episode: 175, reward -99.8\n",
      "Episode: 176, reward -99.7\n",
      "Episode: 177, reward -99.9\n",
      "Episode: 178, reward 2.6\n",
      "Episode: 179, reward 12.1\n",
      "Episode: 180, reward -99.9\n",
      "Episode: 181, reward -99.8\n",
      "Episode: 182, reward -99.9\n",
      "Episode: 183, reward -99.8\n",
      "Episode: 184, reward -99.8\n",
      "Episode: 185, reward -99.8\n",
      "Episode: 186, reward -99.9\n",
      "Episode: 187, reward 47.3\n",
      "Episode: 188, reward 69.9\n",
      "Episode: 189, reward -99.9\n",
      "Episode: 190, reward -99.8\n",
      "Episode: 191, reward -99.9\n",
      "Episode: 192, reward -99.8\n",
      "Episode: 193, reward -99.8\n",
      "Episode: 194, reward -99.9\n",
      "Episode: 195, reward -99.9\n",
      "Episode: 196, reward 10.6\n",
      "Episode: 197, reward -99.9\n",
      "Episode: 198, reward 26.5\n",
      "Episode: 199, reward 33.2\n",
      "Episode: 200, reward -99.8\n",
      "Episode: 201, reward -99.9\n",
      "Episode: 202, reward -99.9\n",
      "Episode: 203, reward -99.8\n",
      "Episode: 204, reward 22.0\n",
      "Episode: 205, reward -99.8\n",
      "Episode: 206, reward 62.0\n",
      "Episode: 207, reward -99.8\n",
      "Episode: 208, reward -99.8\n",
      "Episode: 209, reward -99.9\n",
      "Episode: 210, reward -99.9\n",
      "Episode: 211, reward 12.4\n",
      "Episode: 212, reward -99.9\n",
      "Episode: 213, reward -99.9\n",
      "Episode: 214, reward 7.0\n",
      "Episode: 215, reward 47.7\n",
      "Episode: 216, reward 21.2\n",
      "Episode: 217, reward -99.8\n",
      "Episode: 218, reward -99.9\n",
      "Episode: 219, reward -99.8\n",
      "Episode: 220, reward -99.9\n",
      "Episode: 221, reward -99.8\n",
      "Episode: 222, reward 32.4\n",
      "Episode: 223, reward -99.9\n",
      "Episode: 224, reward 43.1\n",
      "Episode: 225, reward 45.9\n",
      "Episode: 226, reward -99.9\n",
      "Episode: 227, reward 37.7\n",
      "Episode: 228, reward -99.9\n",
      "Episode: 229, reward 48.8\n",
      "Episode: 230, reward -99.9\n",
      "Episode: 231, reward 0.2\n",
      "Episode: 232, reward -99.9\n",
      "Episode: 233, reward 45.5\n",
      "Episode: 234, reward -99.8\n",
      "Episode: 235, reward -99.8\n",
      "Episode: 236, reward -99.9\n",
      "Episode: 237, reward -99.9\n",
      "Episode: 238, reward -99.9\n",
      "Episode: 239, reward -99.9\n",
      "Episode: 240, reward -99.8\n",
      "Episode: 241, reward 42.1\n",
      "Episode: 242, reward 36.3\n",
      "Episode: 243, reward 29.0\n",
      "Episode: 244, reward 52.5\n",
      "Episode: 245, reward 18.4\n",
      "Episode: 246, reward -99.8\n",
      "Episode: 247, reward 15.8\n",
      "Episode: 248, reward -99.9\n",
      "Episode: 249, reward 60.2\n",
      "Episode: 250, reward 13.7\n",
      "Episode: 251, reward 37.4\n",
      "Episode: 252, reward -99.8\n",
      "Episode: 253, reward -99.9\n",
      "Episode: 254, reward -99.9\n",
      "Episode: 255, reward -99.7\n",
      "Episode: 256, reward -99.8\n",
      "Episode: 257, reward -99.9\n",
      "Episode: 258, reward -99.8\n",
      "Episode: 259, reward -99.9\n",
      "Episode: 260, reward 63.7\n",
      "Episode: 261, reward 26.8\n",
      "Episode: 262, reward 37.6\n",
      "Episode: 263, reward -99.9\n",
      "Episode: 264, reward 44.3\n",
      "Episode: 265, reward -99.8\n",
      "Episode: 266, reward -99.8\n",
      "Episode: 267, reward -99.9\n",
      "Episode: 268, reward -99.8\n",
      "Episode: 269, reward -99.9\n",
      "Episode: 270, reward 54.5\n",
      "Episode: 271, reward -99.9\n",
      "Episode: 272, reward -99.9\n",
      "Episode: 273, reward 62.7\n",
      "Episode: 274, reward -99.9\n",
      "Episode: 275, reward -99.9\n",
      "Episode: 276, reward -99.9\n",
      "Episode: 277, reward -99.8\n",
      "Episode: 278, reward 54.7\n",
      "Episode: 279, reward 32.1\n",
      "Episode: 280, reward 19.8\n",
      "Episode: 281, reward -99.9\n",
      "Episode: 282, reward 1.1\n",
      "Episode: 283, reward 42.3\n",
      "Episode: 284, reward 22.3\n",
      "Episode: 285, reward -99.9\n",
      "Episode: 286, reward -99.8\n",
      "Episode: 287, reward -99.9\n",
      "Episode: 288, reward 33.5\n",
      "Episode: 289, reward -99.9\n",
      "Episode: 290, reward -99.9\n",
      "Episode: 291, reward -99.9\n",
      "Episode: 292, reward -99.8\n",
      "Episode: 293, reward 32.3\n",
      "Episode: 294, reward -99.9\n",
      "Episode: 295, reward 46.1\n",
      "Episode: 296, reward 48.2\n",
      "Episode: 297, reward 49.0\n",
      "Episode: 298, reward -99.9\n",
      "Episode: 299, reward -99.8\n",
      "Episode: 300, reward -99.9\n",
      "Episode: 301, reward -99.9\n",
      "Episode: 302, reward -99.9\n",
      "Episode: 303, reward -99.9\n",
      "Episode: 304, reward -99.9\n",
      "Episode: 305, reward 34.0\n",
      "Episode: 306, reward 38.5\n",
      "Episode: 307, reward 30.7\n",
      "Episode: 308, reward 78.8\n",
      "Episode: 309, reward 49.0\n",
      "Episode: 310, reward -99.8\n",
      "Episode: 311, reward 22.9\n",
      "Episode: 312, reward 72.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 313, reward -99.9\n",
      "Episode: 314, reward -99.9\n",
      "Episode: 315, reward -99.9\n",
      "Episode: 316, reward 18.9\n",
      "Episode: 317, reward -99.8\n",
      "Episode: 318, reward 31.6\n",
      "Episode: 319, reward -99.9\n",
      "Episode: 320, reward -99.8\n",
      "Episode: 321, reward -99.8\n",
      "Episode: 322, reward -99.9\n",
      "Episode: 323, reward -99.8\n",
      "Episode: 324, reward -99.9\n",
      "Episode: 325, reward -99.9\n",
      "Episode: 326, reward 34.6\n",
      "Episode: 327, reward -99.9\n",
      "Episode: 328, reward -99.9\n",
      "Episode: 329, reward 16.5\n",
      "Episode: 330, reward -99.9\n",
      "Episode: 331, reward 24.0\n",
      "Episode: 332, reward -99.9\n",
      "Episode: 333, reward -99.8\n",
      "Episode: 334, reward -99.9\n",
      "Episode: 335, reward -99.9\n",
      "Episode: 336, reward 43.0\n",
      "Episode: 337, reward -99.9\n",
      "Episode: 338, reward 24.6\n",
      "Episode: 339, reward 24.7\n",
      "Episode: 340, reward 1.6\n",
      "Episode: 341, reward -99.9\n",
      "Episode: 342, reward -99.9\n",
      "Episode: 343, reward -99.9\n",
      "Episode: 344, reward -99.9\n",
      "Episode: 345, reward -99.8\n",
      "Episode: 346, reward -99.9\n",
      "Episode: 347, reward -99.9\n",
      "Episode: 348, reward -99.9\n",
      "Episode: 349, reward 72.1\n",
      "Episode: 350, reward -99.9\n",
      "Episode: 351, reward -99.9\n",
      "Episode: 352, reward -99.8\n",
      "Episode: 353, reward -99.9\n",
      "Episode: 354, reward -99.9\n",
      "Episode: 355, reward -99.9\n",
      "Episode: 356, reward -99.9\n",
      "Episode: 357, reward 27.9\n",
      "Episode: 358, reward 57.8\n",
      "Episode: 359, reward -99.9\n",
      "Episode: 360, reward 24.1\n",
      "Episode: 361, reward -99.9\n",
      "Episode: 362, reward -99.9\n",
      "Episode: 363, reward -99.9\n",
      "Episode: 364, reward 43.4\n",
      "Episode: 365, reward -99.9\n",
      "Episode: 366, reward -99.9\n",
      "Episode: 367, reward -99.9\n",
      "Episode: 368, reward -99.9\n",
      "Episode: 369, reward -99.9\n",
      "Episode: 370, reward -99.8\n",
      "Episode: 371, reward -99.9\n",
      "Episode: 372, reward -99.9\n",
      "Episode: 373, reward 9.0\n",
      "Episode: 374, reward 47.8\n",
      "Episode: 375, reward 37.6\n",
      "Episode: 376, reward -99.8\n",
      "Episode: 377, reward -99.9\n",
      "Episode: 378, reward -99.9\n",
      "Episode: 379, reward -99.9\n",
      "Episode: 380, reward 11.4\n",
      "Episode: 381, reward -99.9\n",
      "Episode: 382, reward 30.9\n",
      "Episode: 383, reward 60.0\n",
      "Episode: 384, reward 4.6\n",
      "Episode: 385, reward -99.9\n",
      "Episode: 386, reward -99.9\n",
      "Episode: 387, reward 40.6\n",
      "Episode: 388, reward 39.2\n",
      "Episode: 389, reward -99.9\n",
      "Episode: 390, reward 36.7\n",
      "Episode: 391, reward 58.2\n",
      "Episode: 392, reward -99.9\n",
      "Episode: 393, reward 0.2\n",
      "Episode: 394, reward -99.9\n",
      "Episode: 395, reward -99.9\n",
      "Episode: 396, reward -99.9\n",
      "Episode: 397, reward -99.9\n",
      "Episode: 398, reward -99.9\n",
      "Episode: 399, reward -99.9\n",
      "Episode: 400, reward -99.8\n",
      "Episode: 401, reward -99.9\n",
      "Episode: 402, reward 25.7\n",
      "Episode: 403, reward -99.9\n",
      "Episode: 404, reward -99.9\n",
      "Episode: 405, reward 26.2\n",
      "Episode: 406, reward -99.9\n",
      "Episode: 407, reward -99.9\n",
      "Episode: 408, reward -99.9\n",
      "Episode: 409, reward 53.0\n",
      "Episode: 410, reward -99.8\n",
      "Episode: 411, reward 44.0\n",
      "Episode: 412, reward 19.9\n",
      "Episode: 413, reward -99.9\n",
      "Episode: 414, reward -99.9\n",
      "Episode: 415, reward -99.9\n",
      "Episode: 416, reward -99.9\n",
      "Episode: 417, reward -99.9\n",
      "Episode: 418, reward -99.9\n",
      "Episode: 419, reward -99.9\n",
      "Episode: 420, reward -99.9\n",
      "Episode: 421, reward -99.9\n",
      "Episode: 422, reward -99.9\n",
      "Episode: 423, reward -99.9\n",
      "Episode: 424, reward -99.9\n",
      "Episode: 425, reward 39.5\n",
      "Episode: 426, reward -99.9\n",
      "Episode: 427, reward 18.9\n",
      "Episode: 428, reward -99.9\n",
      "Episode: 429, reward -99.9\n",
      "Episode: 430, reward -99.9\n",
      "Episode: 431, reward 21.4\n",
      "Episode: 432, reward -99.9\n",
      "Episode: 433, reward 18.2\n",
      "Episode: 434, reward -99.9\n",
      "Episode: 435, reward 37.7\n",
      "Episode: 436, reward 68.5\n",
      "Episode: 437, reward -99.9\n",
      "Episode: 438, reward -99.9\n",
      "Episode: 439, reward 6.0\n",
      "Episode: 440, reward -99.9\n",
      "Episode: 441, reward 17.9\n",
      "Episode: 442, reward 13.4\n",
      "Episode: 443, reward -99.9\n",
      "Episode: 444, reward -99.9\n",
      "Episode: 445, reward -99.9\n",
      "Episode: 446, reward 12.1\n",
      "Episode: 447, reward 39.5\n",
      "Episode: 448, reward -99.9\n",
      "Episode: 449, reward 23.7\n",
      "Episode: 450, reward 26.3\n",
      "Episode: 451, reward -99.8\n",
      "Episode: 452, reward -99.8\n",
      "Episode: 453, reward -99.9\n",
      "Episode: 454, reward -99.8\n",
      "Episode: 455, reward 15.3\n",
      "Episode: 456, reward 76.9\n",
      "Episode: 457, reward -99.9\n",
      "Episode: 458, reward -99.9\n",
      "Episode: 459, reward -99.9\n",
      "Episode: 460, reward -99.9\n",
      "Episode: 461, reward 10.0\n",
      "Episode: 462, reward -99.9\n",
      "Episode: 463, reward -99.9\n",
      "Episode: 464, reward 27.7\n",
      "Episode: 465, reward 3.9\n",
      "Episode: 466, reward 61.2\n",
      "Episode: 467, reward -99.9\n",
      "Episode: 468, reward -99.9\n",
      "Episode: 469, reward -99.9\n",
      "Episode: 470, reward 39.4\n",
      "Episode: 471, reward 42.5\n",
      "Episode: 472, reward 25.8\n",
      "Episode: 473, reward -99.9\n",
      "Episode: 474, reward -99.9\n",
      "Episode: 475, reward -99.9\n",
      "Episode: 476, reward -99.9\n",
      "Episode: 477, reward -99.9\n",
      "Episode: 478, reward -99.9\n",
      "Episode: 479, reward 53.7\n",
      "Episode: 480, reward -99.9\n",
      "Episode: 481, reward 34.6\n",
      "Episode: 482, reward -99.9\n",
      "Episode: 483, reward 25.4\n",
      "Episode: 484, reward -99.9\n",
      "Episode: 485, reward -99.9\n",
      "Episode: 486, reward 33.2\n",
      "Episode: 487, reward -99.9\n",
      "Episode: 488, reward -99.9\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-352a485c085b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mReturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode: %d, reward %.1f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy = PolicyNetwork()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "max_episodes = 1000\n",
    "gamma = 0.9\n",
    "reward_all = []\n",
    "for episode in range(max_episodes):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        mu, sigma_ = policy(torch.FloatTensor([state]))\n",
    "        sigma_ = F.softplus(sigma_) # sigma_ > 0\n",
    "        action = mu + sigma_.sqrt() * torch.randn(1) # sampling action from normal distribution\n",
    "        prob = (-1 * (action - mu).pow(2)/(2 * sigma_)).exp() / (2 * sigma_ * math.pi).sqrt()\n",
    "        log_prob = prob.log()\n",
    "        action = F.tanh(action)\n",
    "        state, reward, done, _ = env.step([action.item()])\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "#         print(action, reward)\n",
    "        if done:\n",
    "            Return = torch.zeros(1,1)\n",
    "            for reward, log_prob in zip(reversed(rewards), reversed(log_probs)):\n",
    "                Return = reward + gamma * Return\n",
    "                loss = - log_prob * Return\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            print(\"Episode: %d, reward %.1f\" % (episode, np.sum(rewards)))\n",
    "            reward_all.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "            log_probs = []\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Advantage Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StateValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ab2a1ce27422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mReturn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mReturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                 \u001b[0madvantage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReturn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_state_value\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstate_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m                 \u001b[0mpolicy_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_prob\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madvantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "policy_network = PolicyNetwork()\n",
    "state_value_network = StateValueNetwork()\n",
    "optimizer_policy = optim.Adam(policy_network.parameters(), lr=1e-3)\n",
    "optimizer_state_value = optim.Adam(state_value_network.parameters(), lr=1e-4)\n",
    "max_episodes = 1000\n",
    "num_steps = 200\n",
    "gamma = 0.9\n",
    "reward_all = []\n",
    "for episode in range(max_episodes):\n",
    "    rewards = []\n",
    "    log_probs = []\n",
    "    state_values = []\n",
    "    state = env.reset()\n",
    "    step = 0\n",
    "    while True:\n",
    "        mu, sigma_ = policy(torch.FloatTensor([state]))\n",
    "        sigma_ = F.softplus(sigma_) # sigma_ > 0\n",
    "        action = mu + sigma_.sqrt() * torch.randn(1) # sampling action from normal distribution\n",
    "        prob = (-1 * (action - mu).pow(2)/(2 * sigma_)).exp() / (2 * sigma_ * math.pi).sqrt()\n",
    "        log_prob = prob.log()\n",
    "        action = F.tanh(action)\n",
    "        state_value = state_value_network(torch.FloatTensor([state]))\n",
    "        state, reward, done, _ = env.step([action.item()])\n",
    "        rewards.append(reward)\n",
    "        log_probs.append(log_prob)\n",
    "        state_values.append(state_value)\n",
    "        step += 1\n",
    "        \n",
    "        if step >= num_steps or done:\n",
    "            Return = torch.zeros(1,1)\n",
    "            if done:\n",
    "                next_state_value = torch.zeros(1,1)\n",
    "            else:\n",
    "                mu, sigma_ = policy(torch.FloatTensor([state]))\n",
    "                sigma_ = F.softplus(sigma_) # sigma_ > 0\n",
    "                action = mu + sigma_.sqrt() * torch.randn(1)\n",
    "                action = F.tanh(action)\n",
    "                state, reward, done, _ = env.step([action.item()])\n",
    "                next_state_value = state_value_network(torch.FloatTensor([state]))\n",
    "            for reward, log_prob, state_value in zip(reversed(rewards), reversed(log_probs), reversed(state_values)):\n",
    "                Return = reward + gamma * Return\n",
    "                advantage = Return + (gamma ** step) * next_state_value - state_value\n",
    "                policy_loss = - log_prob * advantage\n",
    "                \n",
    "                optimizer_state_value.zero_grad()\n",
    "                optimizer_policy.zero_grad()\n",
    "                state_value_loss = advantage.pow(2) ## Advantage * Advantage ??? Advantage doesn't work!\n",
    "                state_value_loss.backward(retain_graph=True)\n",
    "                policy_loss.backward(retain_graph=True)\n",
    "                optimizer_state_value.step()\n",
    "                optimizer_policy.step()\n",
    "        if done:\n",
    "            print(\"Episode: %d, reward %.1f\" % (episode, np.sum(rewards)))\n",
    "            reward_all.append(np.sum(rewards))\n",
    "            rewards = []\n",
    "            log_probs = []\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
